{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EASYOCR = 1\n",
    "\n",
    "if 1 == 0:\n",
    "    %pip install easyocr  \n",
    "    %pip install pytesseract \n",
    "    %pip install opencv-python\n",
    "    %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easyocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_EASYOCR \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "if USE_EASYOCR == 1:\n",
    "    import easyocr\n",
    "else:\n",
    "    import pytesseract\n",
    "\n",
    "model_path = 'model.tflite'\n",
    "ocr_config = r'-l ben --oem 3 --psm 11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels into a list\n",
    "classes = []\n",
    "with open('labels.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    if 'str' in line or line == '': break\n",
    "    lbl = line.replace('\\n', '')\n",
    "    classes.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  return resized_img, original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, image):\n",
    "  \"\"\"Set the input tensor.\"\"\"\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_tensor(interpreter, index):\n",
    "  \"\"\"Retur the output tensor at the given index.\"\"\"\n",
    "  output_details = interpreter.get_output_details()[index]\n",
    "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "  # Feed the input image to the model\n",
    "  set_input_tensor(interpreter, image)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  scores = get_output_tensor(interpreter, 0)\n",
    "  boxes = get_output_tensor(interpreter, 1)\n",
    "  count = int(get_output_tensor(interpreter, 2))\n",
    "  classes = get_output_tensor(interpreter, 3)\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path, \n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  print('DETECTED NUMBER PLATE:')\n",
    "  print('----------------------') \n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute \n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * (original_image_np.shape[1] - 5))\n",
    "    xmax = int(xmax * (original_image_np.shape[1] + 10))\n",
    "    ymin = int(ymin * (original_image_np.shape[0] - 5))\n",
    "    ymax = int(ymax * (original_image_np.shape[0] + 10))\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "    \n",
    "    # crop image and apply OCR process\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    cropped = original_image_np[ymin:ymin+h, xmin:xmin+w]\n",
    "    cropped = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "    (_, blackWhite) = cv2.threshold(cropped, 100, 255, cv2.THRESH_BINARY)\n",
    "    detected_img = Image.fromarray(blackWhite)\n",
    "    detected_img.save('detected_plate.jpg')\n",
    "    \n",
    "    if USE_EASYOCR == 1:\n",
    "      reader = easyocr.Reader(['bn'])\n",
    "      result = reader.readtext('detected_plate.jpg', paragraph=\"False\")\n",
    "      print(result[0][1])\n",
    "    else:\n",
    "      number = pytesseract.image_to_string(blackWhite, config=ocr_config)\n",
    "      print(number)\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [255, 255, 0]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 1)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_PLAIN, 1.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_URL = \"sample/car3.jpg\"\n",
    "DETECTION_THRESHOLD = 0.5 \n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    INPUT_IMAGE_URL, \n",
    "    interpreter, \n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4fde45515710cbe4f4cf44a8ddef1b298277709bd6c5462499553af68a98f2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
